# -*- coding: utf-8 -*-
"""LSTM Encoder-Decoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSNuQinwiDDW48bmibUeKkQ68xpbmJJp
"""

import numpy as np
import random
import os, errno
import sys
from tqdm import trange

import torch
from torch.nn import LSTM
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import torchvision.models as models

"""## Encoder"""

class Image_Encoder(nn.Module):

    def __init__(self, embed_dim):

        super(Image_Encoder, self).__init__()
        self.model = models.vgg19(pretrained=True)
        in_features = self.model.classifier[-1].in_features
        self.model.classifier = nn.Sequential(*list(self.model.classifier.children())[:-1]) # remove vgg19 last layer
        self.fc = nn.Linear(in_features, embed_dim)

    def forward(self, image):

        with torch.no_grad():
            img_feature = self.model(image) # (batch, channel, height, width)
        img_feature = self.fc(img_feature)

        l2_norm = F.normalize(img_feature, p=2, dim=1).detach()
        return l2_norm

class Question_Encoder(nn.Module):

    def __init__(self, qu_vocab_size, word_embed, hidden_size, num_hidden, qu_feature_size):

        super(Question_Encoder, self).__init__()
        self.word_embedding = nn.Embedding(qu_vocab_size, word_embed)
        self.tanh = nn.Tanh()
        self.lstm = nn.LSTM(word_embed, hidden_size, num_hidden) # input_feature, hidden_feature, num_layer
        self.fc = nn.Linear(2*num_hidden*hidden_size, qu_feature_size)

    def forward(self, question):

        qu_embedding = self.word_embedding(question)                # (batchsize, qu_length=30, word_embed=300)
        qu_embedding = self.tanh(qu_embedding)
        qu_embedding = qu_embedding.transpose(0, 1)                 # (qu_length=30, batchsize, word_embed=300)
        _, (hidden, cell) = self.lstm(qu_embedding)                 # (num_layer=2, batchsize, hidden_size=1024)
        qu_feature = torch.cat((hidden, cell), dim=2)               # (num_layer=2, batchsize, 2*hidden_size=1024)
        qu_feature = qu_feature.transpose(0, 1)                     # (batchsize, num_layer=2, 2*hidden_size=1024)
        qu_feature = qu_feature.reshape(qu_feature.size()[0], -1)   # (batchsize, 2*num_layer*hidden_size=2048)
        qu_feature = self.tanh(qu_feature)
        qu_feature = self.fc(qu_feature)                            # (batchsize, qu_feature_size=1024)

        return qu_feature

class LSTM_Encoder(nn.Module):

    def __init__(self, feature_size, qu_vocab_size, word_embed, hidden_size, num_hidden):

        super(LSTM_Encoder, self).__init__()
        self.img_encoder = Image_Encoder(feature_size)
        self.qu_encoder = Question_Encoder(qu_vocab_size, word_embed, hidden_size, num_hidden, feature_size)
        self.dropout = nn.Dropout(0.5)
        self.tanh = nn.Tanh()
        self.fc = nn.Linear(feature_size, 512)


    def forward(self, image, question):

        img_feature = self.img_encoder(image)               # (batchsize, feature_size=1024)
        qst_feature = self.qu_encoder(question)
        combined_feature = img_feature * qst_feature
        combined_feature = self.dropout(combined_feature)
        combined_feature = self.tanh(combined_feature)
        combined_feature = self.fc(combined_feature)       # (batchsize, 512)

        return combined_feature

"""## Decoder"""

class LSTM_Decoder(nn.Module):
    def __init__(self, embedding, input_size=512, hidden_size=512, output_size=512, num_layers=2, bidirectional=False, prob=0.5):
        super().__init__()
        self.embedding = embedding
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.input_size = input_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        
        self.dropout = nn.Dropout(prob)
        
        dropout = 0 if self.num_layers == 1 else prob
        
        self.lstm = LSTM(input_size=input_size, hidden_size=self.hidden_size, 
                        bidirectional=self.bidirectional, 
                        num_layers=self.num_layers, dropout = dropout)
            
        D = 2 if self.lstm.bidirectional==True else 1
        self.Linear = nn.Linear(D*self.hidden_size, output_size)
    
    def forward(self, x, hidden):
        """
            # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length
            # is 1 here because we are sending in a single word and not a sentence
        """
        x = x.unsqueeze(0)
        embedding = self.dropout(self.embedding(x))
        # embedding shape: (1, N, embedding_size)
        
        outputs, hidden = self.lstm(embedding, hidden)
        # outputs shape: (1, N, hidden_size)
        
        predictions = self.Linear(outputs)
        
        # predictions shape: (1, N, length_target_vocabulary) to send it to
        # loss function we want it to be (N, length_target_vocabulary) so we're
        # just gonna remove the first dim
        predictions = predictions.squeeze(0)

        return predictions, hidden

"""## PreProcess"""

import json
import os
import re
from collections import defaultdict

src_dir = "/HDD-1_data/dataset/VQA-v2"
saving_dir = "../preprocess"

top_answer = 1000

def make_q_vocab():

    dataset = os.listdir(src_dir + '/Questions')
    regex = re.compile(r'(\W+)')
    q_vocab = []
    for file in dataset:

        path = os.path.join(src_dir, 'Questions', file)
        with open(path, 'r') as f:
            q_data = json.load(f)
        question = q_data['questions']
        for idx, quest in enumerate(question):

            split = regex.split(quest['question'].lower())
            tmp = [w.strip() for w in split if len(w.strip()) > 0]
            q_vocab.extend(tmp)

    q_vocab = list(set(q_vocab))
    q_vocab.sort()
    q_vocab.insert(0, '<pad>')
    q_vocab.insert(1, '<unk>')

    if not os.path.exists(saving_dir): os.makedirs(saving_dir)
    with open(saving_dir + '/question_vocabs.txt', 'w') as f:
        f.writelines([v+'\n' for v in q_vocab])

    print(f"total word:{len(q_vocab)}")

def make_a_vocab(top_answer):

    answers = defaultdict(lambda :0)
    dataset = os.listdir(src_dir + '/Annotations')
    for file in dataset:
        path = os.path.join(src_dir, 'Annotations', file)
        with open(path, 'r') as f:
            data = json.load(f)
        annotations = data['annotations']
        for label in annotations:
            for ans in label['answers']:
                vocab = ans['answer']
                if re.search(r'[^\w\s]', vocab):
                    continue
                answers[vocab] += 1

    answers = sorted(answers, key=answers.get, reverse= True) # sort by numbers
    top_answers = ['<unk>'] + answers[:top_answer-1]
    with open(saving_dir + '/annotation_vocabs.txt', 'w') as f :
        f.writelines([ans+'\n' for ans in top_answers])

    print(f'The number of total words of answers: {len(answers)}')
    print(f'Keep top {top_answers} answers into vocab' )

make_q_vocab()
make_a_vocab(top_answer)

import os
import re
import json
import glob
import numpy as np

image_dir = "../data/resize_image"
annotation_dir = "/HDD-1_data/dataset/VQA-v2/Annotations"
question_dir = "/HDD-1_data/dataset/VQA-v2/Questions"
output_dir = "../data"

def preprocessing(question, annotation_dir, image_dir, labeled):

    with open(question, 'r') as f:
        data = json.load(f)
        questions = data['questions']
        if data['data_subtype'] == 'test-dev2015':
            filename = 'test2015'   # images of test-dev are same as test images
        else:
            filename = data['data_subtype']

    if labeled:
        template = annotation_dir + f'/*{filename}*.json'
        annotation_path = glob.glob(template)[0]
        with open(annotation_path) as f:
            annotations = json.load(f)['annotations']
        question_dict = {ans['question_id']: ans for ans in annotations}

    match_top_ans.unk_ans = 0
    dataset = [None]*len(questions)
    for idx, qu in enumerate(questions):
        if (idx+1) % 10000 == 0:
            print(f'processing {data["data_subtype"]} data: {idx+1}/{len(questions)}')
        qu_id = qu['question_id']
        qu_sentence = qu['question']
        qu_tokens = tokenizer(qu_sentence)
        img_id = qu['image_id']
        img_name = 'COCO_' + filename + '_{:0>12d}.jpg'.format(img_id)
        img_path = os.path.join(image_dir, filename, img_name)

        info = {'img_name': img_name,
                'img_path': img_path,
                'qu_sentence': qu_sentence,
                'qu_tokens': qu_tokens,
                'qu_id': qu_id}

        if labeled:

            annotation_ans = question_dict[qu_id]['answers']
            all_ans, valid_ans = match_top_ans(annotation_ans)
            info['all_ans'] = all_ans
            info['valid_ans'] = valid_ans

        dataset[idx] = info

    print(f'total {match_top_ans.unk_ans} out of {len(questions)} answers are <unk>')
    return dataset

def tokenizer(sentence):

    regex = re.compile(r'(\W+)')
    tokens = regex.split(sentence.lower())
    tokens = [w.strip() for w in tokens if len(w.strip()) > 0]
    return tokens

def match_top_ans(annotation_ans):

    annotation_dir = output_dir + '/annotation_vocabs.txt'
    if "top_ans" not in match_top_ans.__dict__:
        with open(annotation_dir, 'r') as f:
            match_top_ans.top_ans = {line.strip() for line in f}
    annotation_ans = {ans['answer'] for ans in annotation_ans}
    valid_ans = match_top_ans.top_ans & annotation_ans

    if len(valid_ans) == 0:
        valid_ans = ['<unk>']
        match_top_ans.unk_ans += 1

    return annotation_ans, valid_ans

def main():

    processed_data = {}
    for file in os.listdir(question_dir):

        datatype = file[20:-19]
        labeled = False if "test" in datatype else True
        question = os.path.join(question_dir, file)
        processed_data[datatype] = preprocessing(question, annotation_dir, image_dir, labeled)

    processed_data['train-val'] = processed_data['train'] + processed_data['val']
    for key, value in processed_data.items():
        np.save(os.path.join(output_dir, f'{key}.npy'), np.array(value))

main()

import os
import argparse
from PIL import Image


def resize_image(image, size):
    """Resize an image to the given size."""
    return image.resize(size, Image.ANTIALIAS)


def resize_images(input_dir, output_dir, size):
    """Resize the images in 'input_dir' and save into 'output_dir'."""
    for idir in os.scandir(input_dir):
        if not idir.is_dir():
            continue
        if not os.path.exists(output_dir+'/'+idir.name):
            os.makedirs(output_dir+'/'+idir.name)    
        images = os.listdir(idir.path)
        n_images = len(images)
        for iimage, image in enumerate(images):
            try:
                with open(os.path.join(idir.path, image), 'r+b') as f:
                    with Image.open(f) as img:
                        img = resize_image(img, size)
                        img.save(os.path.join(output_dir+'/'+idir.name, image), img.format)
            except(IOError, SyntaxError) as e:
                pass
            if (iimage+1) % 1000 == 0:
                print("[{}/{}] Resized the images and saved into '{}'."
                      .format(iimage+1, n_images, output_dir+'/'+idir.name))
            
            
def main(args):

    input_dir = args.input_dir
    output_dir = args.output_dir
    image_size = [args.image_size, args.image_size]
    resize_images(input_dir, output_dir, image_size)

parser = argparse.ArgumentParser()

parser.add_argument('--input_dir', type=str, default="/HDD-1_data/dataset/VQA-v2/Images/mscoco",
                    help='directory for input images (unresized images)')

parser.add_argument('--output_dir', type=str, default='../data/resize_image',
                    help='directory for output images (resized images)')

parser.add_argument('--image_size', type=int, default=224,
                    help='size of images after resizing')

args = parser.parse_args()

main(args)

"""## Dataset"""

import numpy as np
import os

from torch.utils.data import Dataset
from torchvision import transforms
from torch.utils.data import DataLoader
from PIL import Image

INPUT_DIR = '../data'

class VQADataset(Dataset):

    def __init__(self, input_dir, input_file, max_qu_len = 30, transform = None):

        self.input_data = np.load(os.path.join(input_dir, input_file), allow_pickle=True)
        self.qu_vocab = Vocab(input_dir+'/question_vocabs.txt')
        self.ans_vocab = Vocab(input_dir+'/annotation_vocabs.txt')
        self.max_qu_len = max_qu_len
        self.labeled = True if not "test" in input_file else False
        self.transform = transform

    def __getitem__(self, idx):

        path = self.input_data[idx]['img_path']
        img = np.array(Image.open(path).convert('RGB'))
        qu_id = self.input_data[idx]['qu_id']
        qu_tokens = self.input_data[idx]['qu_tokens']
        qu2idx = np.array([self.qu_vocab.word2idx('<pad>')] * self.max_qu_len)
        qu2idx[:len(qu_tokens)] = [self.qu_vocab.word2idx(token) for token in qu_tokens]
        sample = {'image': img, 'question': qu2idx, 'question_id': qu_id}

        if self.labeled:
            ans2idx = [self.ans_vocab.word2idx(ans) for ans in self.input_data[idx]['valid_ans']]
            ans2idx = np.random.choice(ans2idx)
            sample['answer'] = ans2idx

        if self.transform:
            sample['image'] = self.transform(sample['image'])

        return sample

    def __len__(self):

        return len(self.input_data)

def data_loader(input_dir, batch_size, max_qu_len, num_worker):

    transform = transforms.Compose([
        transforms.ToTensor(),  # convert to (C,H,W) and [0,1]
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # mean=0; std=1
    ])

    vqa_dataset = {
        'train': VQADataset(
            input_dir=input_dir,
            input_file='train.npy',
            max_qu_len=max_qu_len,
            transform=transform),
        'val': VQADataset(
            input_dir=input_dir,
            input_file='val.npy',
            max_qu_len=max_qu_len,
            transform=transform)
    }

    dataloader = {
        key: DataLoader(
            dataset=vqa_dataset[key],
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_worker)
        for key in ['train', 'val']
    }

    return dataloader

class Vocab:

    def __init__(self, vocab_file):

        self.vocab = self.load_vocab(vocab_file)
        self.vocab2idx = {vocab: idx for idx, vocab in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)

    def load_vocab(self, vocab_file):

        with open(vocab_file) as f:
            vocab = [v.strip() for v in f]

        return vocab

    def word2idx(self, vocab):

        if vocab in self.vocab2idx:
            return self.vocab2idx[vocab]
        else:
            return self.vocab2idx['<unk>']

    def idx2word(self, idx):

        return self.vocab[idx]

"""## Model"""

BATCH_SIZE = 150
MAX_QU_LEN = 30
NUM_WORKER = 8
FEATURE_SIZE, WORD_EMBED = 1024, 300
NUM_HIDDEN, HIDDEN_SIZE = 2, 512
LEARNING_RATE, STEP_SIZE, GAMMA = 0.001, 10, 0.1
EPOCH = 50
DATA_DIR = '../data'

dataloader = data_loader(input_dir=DATA_DIR, batch_size=BATCH_SIZE, max_qu_len=MAX_QU_LEN, num_worker=NUM_WORKER)
qu_vocab_size = dataloader['train'].dataset.qu_vocab.vocab_size

class LSTM_Encoder_Decoder(torch.nn.Module):
    def __init__(self, num_layers=2, bidirectional=False, prob=0.5, freeze_encoder=True):
        super().__init__()
      
        self.encoder = LSTM_Encoder(feature_size=FEATURE_SIZE, qu_vocab_size=qu_vocab_size, word_embed=WORD_EMBED, 
                                    hidden_size=HIDDEN_SIZE, num_hidden=NUM_HIDDEN)
        #freeze encoder
        if freeze_encoder:
            for p in self.encoder.parameters():
                p.requires_grad = False
        
        self.embedding_layer = nn.Embedding(qu_vocab_size=qu_vocab_size, word_embed=WORD_EMBED)

        self.decoder = LSTM_Decoder(embedding=self.embedding_layer,
                            output_size=qu_vocab_size, 
                            num_layers=2, 
                            bidirectional=False, 
                            prob=0.5)
        
        self.start_token = 101 # <cls>
        self.end_token = 102 # <sep>
        
        self.D = 2 if bidirectional==True else 1

    def forward(self, image, question, answer_tokenized = None, teacher_force_ratio=0.5, max_sequence_length=50):
        """
            Train phase forward propagation
        """
        
        batch_size = 150
        target_len = max_sequence_length if answer_tokenized is None else answer_tokenized.shape[0]
        target_vocab_size = self.Tokenizer.vocab_size
        
        outputs = torch.zeros(target_len, batch_size, target_vocab_size).cuda()
        
        #encoder
  
        encoder_output = self.encoder(image, question)
            
        # encoder_output shape: (N, hidden_size) to send it to Decoder as hidden,
        # we want it to be (D*num_layers, N, hidden_size) so we're just gonna expand it.
        
        h = encoder_output.expand(self.D*self.decoder.num_layers, -1, -1)
        # h shape: (D*num_layers, N, hidden_size)
        
        c = torch.zeros(*h.shape).cuda()
        hidden = (h.contiguous(),c.contiguous())
            
        # Send <cls> token to decoder
        x =  torch.tensor([self.start_token]*batch_size).cuda()
        
        for t in range(0, target_len):
            output, hidden = self.decoder(x, hidden)

            # Store next output prediction
            outputs[t] = output

            # Get the best word the decoder predicted (index in the vocabulary)
            best_guess = output.argmax(1)

            # With probability of teacher_force_ratio we take the actual next word
            # otherwise we take the word that the decoder predicted it to be.
            # Teacher Forcing is used so that the model gets used to seeing
            # similar inputs at training and testing time, if teacher forcing is 1
            # then inputs at test time might be completely different than what the
            # network is used to.
            x = answer_tokenized[t] if random.random() < teacher_force_ratio else best_guess

        return outputs

    def save(self, dir_, epoch):
        if not(os.path.exists(dir_)):
            os.makedirs(dir_, exist_ok=True)
        path = os.path.join(dir_, f"{self.name}.{epoch}.torch")
        torch.save(self.state_dict(), path)

"""## Greedydecoder"""

import torch

class GreedyDecoder():
    def __init__(self, tokenizer, pad = 0, sep = 102):
        self.tokenizer = tokenizer
        self.SEP = sep
        self.PAD = pad
    
    def decode_from_logits(self, logits):
        logits = torch.argmax(logits, dim=-1)    
        return logits

    def batch_decode(self, tokens):
        
        # return self.tokenizer.batch_decode(tokens, skip_special_tokens=True)

        sentences = []
        sentences_ids = []
        for i in range(tokens.shape[0]):
            sentence = []
            for j in range(tokens.shape[1]):
                if(tokens[i, j] == self.PAD):
                    continue
                if(tokens[i, j] == self.SEP):
                    break
                sentence.append(tokens[i, j])
            sentences.append(self.tokenizer.decode(sentence, skip_special_tokens=True))
            sentences_ids.append(sentence)
        return sentences, sentences_ids

"""## Utils"""

import math
import numpy as np
import torch
from torch import nn

class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience.
       https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py
    """
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement. 
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
            path (str): Path for the checkpoint to be saved to.
                            Default: 'checkpoint.pt'
            trace_func (function): trace print function.
                            Default: print            
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta
        self.path = path
        self.trace_func = trace_func
    def __call__(self, val_loss):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss)
        elif score < self.best_score + self.delta:
            self.counter += 1
            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss)
            self.counter = 0

    def save_checkpoint(self, val_loss):
        if self.verbose:
            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).')
        self.val_loss_min = val_loss
        
        
class PositionalEncoder(nn.Module):
    """Positional encoding class pulled from the PyTorch documentation tutorial
    on Transformers for seq2seq models:
    https://pytorch.org/tutorials/beginner/transformer_tutorial.html
    """

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoder, self).__init__()

        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float()\
                             * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

"""## Train"""

!pip install torchmetrics
!pip install transformers

import argparse
from cgi import test
import json
import os
import random
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
from torchmetrics import Accuracy, F1Score
from tqdm import tqdm
from transformers import AdamW
import torch.nn.functional as F

BASE_DIR = "./"
CHECKPOINTS_DIR = os.path.join(BASE_DIR, "checkpoints")
class VQA:
    def __init__(self,
                 train_date,
                 model,
                 decoder_type,
                 train_dset,
                 val_dset=None,
                 test_dset=None,
                 use_cuda=True,
                 batch_size=32,
                 epochs=200,
                 lr=0.005,
                 log_every=1,
                 save_every=5, 
                 max_sequence_length=50, 
                 optimizer = 'adam'):
        
        self.model = model
        self.epochs = epochs
        self.batch_size = batch_size
        self.log_every = log_every
        self.train_date_time = train_date
        self.save_every = save_every
        self.decoder_type = decoder_type
        self.max_sequence_length = max_sequence_length
        
        self.train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=pad_batched_sequence)
        self.val_loader = DataLoader(val_dset, batch_size=batch_size, shuffle=False, drop_last=True, collate_fn=pad_batched_sequence)

        if(use_cuda):
            self.model = self.model.cuda()
            
        self.pad_idx = 0
        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_idx)
        
        if optimizer == 'adam':
            self.optim = torch.optim.Adam(list(self.model.parameters()), lr=lr)
        elif optimizer =='sgd':
            self.optim = torch.optim.SGD(list(self.model.parameters()), lr=lr)
        elif optimizer =='adamw':
            self.optim = AdamW(list(self.model.parameters()), lr=lr)
            
        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optim, step_size=10, gamma=0.5)
        self.early_stopping = EarlyStopping(patience=5, verbose=True)
        
        self.f1_score = F1Score(num_classes=self.model.Tokenizer.vocab_size, ignore_index=self.pad_idx, top_k=1, mdmc_average='samplewise')
        self.accuracy = Accuracy(num_classes=self.model.Tokenizer.vocab_size, ignore_index=self.pad_idx, top_k=1, mdmc_average='samplewise')
        
        self.save_dir = os.path.join(CHECKPOINTS_DIR, str(self.train_date_time))
        if not(os.path.exists(self.save_dir)):
            os.makedirs(self.save_dir, exist_ok=True)
        
    def train(self):
        running_loss = running_accuracy = running_accuracy_best = running_f1 = 0
        for epoch in range(self.epochs):
            self.model.train()
            for i, (input_ids, feats, boxes, masks, target) in enumerate(pbar := tqdm(self.train_loader, total=len(self.train_loader))):
                # torch.cuda.empty_cache()
                pbar.set_description(f"Epoch {epoch}")
                loss, batch_acc, batch_f1, _ = self.__step(input_ids, feats, boxes, masks, target, val=False)  
                
                running_loss += loss.item()
                running_accuracy += batch_acc.item()
                running_f1 += batch_f1
                pbar.set_postfix(loss=running_loss/(i+1), accuracy=running_accuracy/(i+1))

            if epoch % self.log_every == self.log_every - 1:                                
                val_loss, val_acc, val_f1, _ = self.__evaluate_validation()
                
                total_data_iterated = self.log_every * len(self.train_loader)
                running_loss /= total_data_iterated
                running_accuracy /= total_data_iterated
                running_f1 /= total_data_iterated
                
                #logging results
                Logger.log(f"Train_{self.train_date_time}", f"Training epoch {epoch}: Train loss {running_loss:.3f}. Val loss: {val_loss:.3f}."
                            + f" Train accuracy {running_accuracy:.3f}. Val accuracy: {val_acc:.3f}. Train F1-Score: {running_f1}. Validation F1-Score: {val_f1}")
                print(f"F1 Score: Train {running_f1}, Validation: {val_f1}")

                if(running_accuracy > running_accuracy_best):
                    self.model.save(self.save_dir, "BEST")
                    running_accuracy_best = running_accuracy
                
                running_loss = running_accuracy = running_f1 = 0
            
            if(epoch % self.save_every == self.save_every - 1):
                self.model.save(self.save_dir, epoch)

            # self.scheduler.step()    
            
            self.early_stopping(val_loss)
            if self.early_stopping.early_stop:
                print("Early stopping")
                break
            
    @torch.no_grad()   
    def __evaluate_validation(self, metric_calculator=False, dset=None):
        print("Validation Evaluations: ")
        self.model.eval()
        val_loss = val_acc = val_f1 = 0

        if(dset):
            loader = DataLoader(dset, batch_size=self.batch_size, shuffle=False, drop_last=True, collate_fn=pad_batched_sequence)
        else:
            loader = self.val_loader
        # define metric calculator if we need extra metric calculation
        if(metric_calculator):
            metric_calculator = MetricCalculator(self.model.embedding_layer)
            # we used greedy decoder as a temporary decode. 
            decoder = GreedyDecoder(self.model.Tokenizer)

            
        
        for i, (image, question, target) in enumerate(pbar := tqdm(loader, total=len(loader))):
            #calculate losses, and logits + necessary metrics for showin during training
            # torch.cuda.empty_cache()
            loss, val_acc_batch, val_f1_batch, logits = self.__step(image, question, target, val=True)
            
            val_loss += loss.item()
            val_acc += val_acc_batch.item()
            val_f1 += val_f1_batch
            pbar.set_postfix(loss=val_loss/(i+1), accuracy=val_acc/(i+1))
            
            #only when we need extra metrics for evaluation!
            if(metric_calculator):
                # using argmax to find the best token!
                preds_tokenized = decoder.decode_from_logits(logits)

                #tokenized sentences without [PAD] and [SEP] tokens. pure sentences!
                pred_sentences_decoded, preds_sentences_ids = decoder.batch_decode(preds_tokenized.permute(1, 0))
                ref_sentences_decoded, ref_sentences_ids = decoder.batch_decode(target.permute(1, 0))
                
                #calculate metrics such as BLEU, ROUGE, BERTSCORE, and others.
                #it accumalates values to be calculated later
                metric_calculator.add_batch(pred_sentences_decoded, ref_sentences_decoded, preds_sentences_ids, ref_sentences_ids)

        val_loss /= len(loader)
        val_acc /= len(loader)
        val_f1 /= len(loader)
        
        #calculate metrics based on the accumelated metrics during evaluation!
        other_metrics = metric_calculator.compute() if metric_calculator else None
        
        return val_loss, val_acc, val_f1, other_metrics
        
    def __step(self, image, question, target, val=False):
        
        teacher_force_ratio = 0 if val else 0.5
        answer_tokenized = None if val else target      
        logits = self.model(image, question, answer_tokenized, teacher_force_ratio, self.max_sequence_length)
        
        # logits shape: (L, N, target_vocab_size)

        if val:
            target = F.pad(input=target, pad=(0, 0, 0, self.max_sequence_length - target.shape[0]), mode='constant', value=self.pad_idx)
            
        loss = self.criterion(logits.permute(1, 2, 0), target.permute(1,0))

        if not(val):
            self.optim.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(self.model.parameters(), 5)
            self.optim.step()

        f1_score = self.f1_score(logits.permute(1,2,0), target.permute(1,0))
        batch_acc = self.accuracy(logits.permute(1,2,0), target.permute(1,0))

        return loss, batch_acc, f1_score, logits

    def evaluate(self, dset, key):
        _ , val_acc, val_f1, other_metrics = self.__evaluate_validation(metric_calculator=True, dset= dset)
        other_metrics["accuracy"] = val_acc
        other_metrics['f1'] = val_f1.cpu().tolist()
        with open(os.path.join(self.save_dir, f"evaluation_{key}.json"), 'w') as fp:
            json.dump(other_metrics, fp)
    
    def load_model(self, key):
        path = os.path.join(self.save_dir, f"{self.model.name}.{key}.torch")
        if not (os.path.exists(path)):
            Logger.log(f"Train_{self.train_date_time}", f"Couldn't load model from {path} ")
            return

        state_dict = torch.load(path)
        self.model.load_state_dict(state_dict)
        
    def predict(self, model_path, dset, key):
        #load model
        if os.path.exists(model_path) == False:
            print(f"Couldn't load model from {model_path}")
            return

        state_dict = torch.load(model_path)
        self.model.load_state_dict(state_dict)
        
        # load dataset
        if(dset):
            loader = DataLoader(dset, batch_size=self.batch_size, shuffle=False, drop_last=True, collate_fn=pad_batched_sequence)
        else:
            loader = self.val_loader
            
        self.model.eval()
        decoder = GreedyDecoder(self.model.Tokenizer)
        questions, pred_sentences, ref_sentences = [], [], []
        
        for i, (image, question, target) in enumerate(pbar := tqdm(loader, total=len(loader))):
            _, _, _, logits = self.__step(image, question, target, val=True)
            
            with torch.no_grad():
                logits = self.model(image, question)
                predict = torch.log_softmax(logits, dim=1)

            predict = torch.argmax(predict, dim=1).tolist()
            predict = [loader.ans_vocab.idx2word(idx) for idx in predict]
            ans_qu_pair = [{'answer': ans, 'question_id': id} for ans, id in zip(predict, question_id)]

            preds_tokenized = decoder.decode_from_logits(logits)
            questions_decoded, _ = decoder.batch_decode(input_ids)
            pred_sentences_decoded, _ = decoder.batch_decode(preds_tokenized.permute(1, 0))
            ref_sentences_decoded, _ = decoder.batch_decode(target.permute(1, 0))
            
            questions.extend(questions_decoded)
            pred_sentences.extend(pred_sentences_decoded)
            ref_sentences.extend(ref_sentences_decoded)
            
            
        model_predictions = [{"question":question, "ref answer": ref_answer, "pred answer":pred_answer} 
                             for question, ref_answer, pred_answer in zip(questions, ref_sentences, pred_sentences)]
              
        with open(os.path.join(os.path.split(model_path)[0], f"model_prediction_{key}.json"), 'w') as fp:
            json.dump(model_predictions, fp)

def parse_args():
    parser = argparse.ArgumentParser()
    # specify mode, options: train, predict:
    parser.add_argument("--mode", default="train", type=str)
    parser.add_argument("--optimizer", default="adam", type=str)
    parser.add_argument("--lr", default=0.005, type=float)
    
    # specify model_path to load for prediction
    parser.add_argument("--model_path", default='', type=str)
    
    #specify seed for reproducing
    parser.add_argument("--seed", default=8956, type=int)
    
    #specify encoder type, options: lxmert, visualbert 
    parser.add_argument("--encoder_type", default="lxmert", type=str)
    
    #specify decoder type, options: rnn, attn-rnn, transformer
    parser.add_argument("--decoder_type", default="rnn", type=str)
    
    #RNN specifications
    parser.add_argument("--rnn_type", default="lstm", type=str) #options: lstm, gru
    parser.add_argument("--num_rnn_layers", default=1, type=int)
    parser.add_argument("--bidirectional", default=False, action="store_true")
    
    # Attention RNN specifications
    parser.add_argument("--attn_type", default="bahdanau", type=str) #options: bahdanau, luong
    # use only when attention type is luong
    parser.add_argument("--attn_method", default="dot", type=str) #options: dot, general, concat
    
    #Transformer specifications
    parser.add_argument("--nheads", default=12, type=int)
    parser.add_argument("--num_transformer_layers", default=6, type=int)

    return parser.parse_args()

args = parse_args()

torch.manual_seed(args.seed)
random.seed(args.seed)
np.random.seed(args.seed)

model = LSTM_Encoder_Decoder()

train_dset = GenVQADataset(model.Tokenizer, 
        annotations = "../fsvqa_data_train_full/annotations.pickle", 
        questions = "../fsvqa_data_train_full/questions.pickle", 
        img_dir = "../img_data")
    
val_dset = GenVQADataset(model.Tokenizer, 
    annotations = "../fsvqa_data_val_full/annotations.pickle", 
    questions = "../fsvqa_data_val_full/questions.pickle", 
    img_dir = "../val_img_data")

test_dset = GenVQADataset(model.Tokenizer,
    annotations = "../fsvqa_data_test_full/annotations.pickle", 
    questions = "../fsvqa_data_test_full/questions.pickle", 
    img_dir = "../val_img_data")


if model:
        vqa = VQA(  
            datetime.now(), 
            model, 
            args.decoder_type, 
            train_dset, 
            val_dset=val_dset, 
            test_dset=test_dset, 
            optimizer=args.optimizer, 
            lr= args.lr
        )
        if args.mode == 'train':
            vqa.train()
            vqa.load_model("BEST")
            vqa.evaluate(val_dset, "VAL")
            vqa.evaluate(test_dset, "TEST")
                
        elif args.mode =='predict':
            vqa.predict(args.model_path, val_dset, "VAL")
        
        elif args.mode == 'evaluate':
            vqa = VQA(
                args.model_path, 
                model, 
                args.decoder_type, 
                train_dset, 
                val_dset=val_dset, 
                test_dset=test_dset
            )
            vqa.load_model("BEST")
            vqa.evaluate(val_dset, "VAL")
            vqa.evaluate(test_dset, "TEST")